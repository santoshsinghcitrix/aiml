{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "hwLgwJFd-Qeu",
      "metadata": {
        "id": "hwLgwJFd-Qeu"
      },
      "source": [
        "# PART ONE\n",
        "\n",
        "# QUESTION:\n",
        "\n",
        "• **DOMAIN**: Digital content management\n",
        "\n",
        "• **CONTEXT**: Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem.\n",
        "\n",
        "• **DATA DESCRIPTION**: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of\n",
        "19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or\n",
        "approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and\n",
        "the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is\n",
        "marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
        "\n",
        "• 8240 \"10s\" blogs (ages 13-17),    \n",
        "• 8086 \"20s\" blogs(ages 23-27) and.    \n",
        "• 2994 \"30s\" blogs (ages 33-47)\n",
        "\n",
        "\n",
        "• For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of\n",
        "common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the\n",
        "date of the following post and links within a post are denoted by the label url link.\n",
        "\n",
        "• **PROJECT OBJECTIVE**: To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case\n",
        "study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable.\n",
        "\n",
        "Steps and tasks: [ Total Score: 40 Marks]\n",
        "\n",
        "1. Read and Analyse Dataset. [5 Marks]\n",
        "\n",
        "    A. Clearly write outcome of data analysis(Minimum 2 points) [2 Marks].  \n",
        "    B. Clean the Structured Data [3 Marks].  \n",
        "        i. Missing value analysis and imputation. [1 Marks]\n",
        "        ii. Eliminate Non-English textual data. [2 Marks]\n",
        "             Hint: Refer ‘langdetect’ library to detect language of the input text)\n",
        "\n",
        "2. Preprocess unstructured data to make it consumable for model training. [5 Marks]\n",
        "\n",
        "    A. Eliminate All special Characters and Numbers [2 Marks].  \n",
        "    B. Lowercase all textual data [1 Marks].  \n",
        "    C. Remove all Stopwords [1 Marks].   \n",
        "    D. Remove all extra white spaces [1 Marks].  \n",
        "\n",
        "3. Build a base Classification model [8 Marks]\n",
        "\n",
        "    A. Create dependent and independent variables [2 Marks].  \n",
        "        Hint: Treat ‘topic’ as a Target variable.\n",
        "    B. Split data into train and test. [1 Marks].  \n",
        "    C. Vectorize data using any one vectorizer. [2 Marks].   \n",
        "    D. Build a base model for Supervised Learning - Classification. [2 Marks].  \n",
        "    E. Clearly print Performance Metrics. [1 Marks].  \n",
        "        Hint: Accuracy, Precision, Recall, ROC-AUC\n",
        "\n",
        "4. Improve Performance of model. [14 Marks].  \n",
        "\n",
        "    A. Experiment with other vectorisers. [4 Marks].  \n",
        "    B. Build classifier Models using other algorithms than base model. [4 Marks].  \n",
        "    C. Tune Parameters/Hyperparameters of the model/s. [4 Marks].  \n",
        "    D. Clearly print Performance Metrics. [2 Marks].  \n",
        "        Hint: Accuracy, Precision, Recall, ROC-AUC.  \n",
        "\n",
        "5. Share insights on relative performance comparison [8 Marks].  \n",
        "\n",
        "    A. Which vectorizer performed better? Probable reason?. [2 Marks].   \n",
        "    B. Which model outperformed? Probable reason? [2 Marks].   \n",
        "    C. Which parameter/hyperparameter significantly helped to improve performance?Probable reason?. [2 Marks].    \n",
        "    D. According to you, which performance metric should be given most importance, why?. [2 Marks]. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lom9r7agYk4_",
      "metadata": {
        "id": "Lom9r7agYk4_"
      },
      "source": [
        "**Mapping the drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "539d11fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539d11fe",
        "outputId": "ea5e0bfc-9c33-4833-a9a9-3def9a00f8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fsrPG-QeYrMt",
      "metadata": {
        "id": "fsrPG-QeYrMt"
      },
      "source": [
        "**Importing the variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "pUEjavzDBmkf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pUEjavzDBmkf",
        "outputId": "cdb972f3-7f60-4297-9b03-334cf1e79965"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cTYmRKELZ-dw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTYmRKELZ-dw",
        "outputId": "b33025e9-b29b-48c7-c786-df24a9a5263d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "QKdvTg0ODRAu",
      "metadata": {
        "id": "QKdvTg0ODRAu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(1)\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "   \n",
        "import statistics\n",
        "import seaborn as sns # For Data Visualization \n",
        "import matplotlib.pyplot as plt # Necessary module for plotting purpose\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, LeakyReLU, Dropout, BatchNormalization, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop,Adagrad\n",
        "\n",
        "# for hyperparameter tuning and KFoldCV\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
        "# from scipy.stats import randint as sp_randint\n",
        "# from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "# getting methods for confusion matrix, F1 score, Accuracy Score etc.\n",
        "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score, classification_report, make_scorer,recall_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DY6Vf7vPB1zd",
      "metadata": {
        "id": "DY6Vf7vPB1zd"
      },
      "source": [
        "-------------------------\n",
        "### 1. Read and Analyse Dataset. [5 Marks]\n",
        "\n",
        "    A. Clearly write outcome of data analysis(Minimum 2 points) [2 Marks].  \n",
        "    B. Clean the Structured Data [3 Marks].  \n",
        "        i. Missing value analysis and imputation. [1 Marks]\n",
        "        ii. Eliminate Non-English textual data. [2 Marks]\n",
        "             Hint: Refer ‘langdetect’ library to detect language of the input text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7U3BLGRNYyPP",
      "metadata": {
        "id": "7U3BLGRNYyPP"
      },
      "source": [
        "**Set project directory**\n",
        "\n",
        "**Unzipping the files and extracting the csv**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "DuD-0MTuBrDI",
      "metadata": {
        "id": "DuD-0MTuBrDI"
      },
      "outputs": [],
      "source": [
        "project_path = \"/content/drive/My Drive/aiml/nlp/project1/\"\n",
        "\n",
        "os.chdir(project_path)\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('blogs.zip', 'r') as zipdata:\n",
        "    data_csv = zipdata.open('blogtext.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dRHs042KZIct",
      "metadata": {
        "id": "dRHs042KZIct"
      },
      "source": [
        "**Read the csv files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "illl8JQVChYb",
      "metadata": {
        "id": "illl8JQVChYb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aJwMyqSlDug4",
      "metadata": {
        "id": "aJwMyqSlDug4"
      },
      "outputs": [],
      "source": [
        "del data_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2laluAAUZR7J",
      "metadata": {
        "id": "2laluAAUZR7J"
      },
      "source": [
        "**Check the column names**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "i8OaUnYmDy8Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8OaUnYmDy8Y",
        "outputId": "46e842e2-258f-4540-8725-03271e3f5b97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'gender', 'age', 'topic', 'sign', 'date', 'text'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21p6JI9kZXuD",
      "metadata": {
        "id": "21p6JI9kZXuD"
      },
      "source": [
        "**We have total 7 columns : 'id', 'gender', 'age', 'topic', 'sign', 'date', 'text'**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kl42t19vZe56",
      "metadata": {
        "id": "kl42t19vZe56"
      },
      "source": [
        "**Checking the data( First 5 rows)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "IXawrQfND2Kw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "IXawrQfND2Kw",
        "outputId": "67ebc8dd-b197-4287-c02a-31aec7e2471d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id gender  age              topic      sign          date  \\\n",
              "0  2059027   male   15            Student       Leo   14,May,2004   \n",
              "1  2059027   male   15            Student       Leo   13,May,2004   \n",
              "2  2059027   male   15            Student       Leo   12,May,2004   \n",
              "3  2059027   male   15            Student       Leo   12,May,2004   \n",
              "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
              "\n",
              "                                                text  \n",
              "0             Info has been found (+/- 100 pages,...  \n",
              "1             These are the team members:   Drewe...  \n",
              "2             In het kader van kernfusie op aarde...  \n",
              "3                   testing!!!  testing!!!            \n",
              "4               Thanks to Yahoo!'s Toolbar I can ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-230b619a-eb0b-4637-9b1b-385ef28c1285\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-230b619a-eb0b-4637-9b1b-385ef28c1285')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-230b619a-eb0b-4637-9b1b-385ef28c1285 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-230b619a-eb0b-4637-9b1b-385ef28c1285');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9G5zP01PZmEn",
      "metadata": {
        "id": "9G5zP01PZmEn"
      },
      "source": [
        "**Checking the shape and info of the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6D0mk95WEfqQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D0mk95WEfqQ",
        "outputId": "e27c246a-c322-42a0-c69e-f4c807151ed4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(681284, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "TQ-HYgUWE63O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ-HYgUWE63O",
        "outputId": "c5955d24-148b-499f-a50f-24a9aa04d4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 681284 entries, 0 to 681283\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   id      681284 non-null  int64 \n",
            " 1   gender  681284 non-null  object\n",
            " 2   age     681284 non-null  int64 \n",
            " 3   topic   681284 non-null  object\n",
            " 4   sign    681284 non-null  object\n",
            " 5   date    681284 non-null  object\n",
            " 6   text    681284 non-null  object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 36.4+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Q2dOg97ZvAG",
      "metadata": {
        "id": "4Q2dOg97ZvAG"
      },
      "source": [
        "**Check if there is null data present on any columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1FknWMjcSUHx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FknWMjcSUHx",
        "outputId": "bfaad707-6c6d-42e6-89e3-c2c472a0e72f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        0\n",
              "gender    0\n",
              "age       0\n",
              "topic     0\n",
              "sign      0\n",
              "date      0\n",
              "text      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4Xt9h6vEOXR",
      "metadata": {
        "id": "O4Xt9h6vEOXR"
      },
      "source": [
        "\n",
        "\n",
        "*   There are 7 columns and 681284 rows of data\n",
        "*   ID and date dont give much value in data, they can be removed.\n",
        "*   Except id and age all the columns are object\n",
        "*   There are no null data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lvy-u6hmaNWg",
      "metadata": {
        "id": "Lvy-u6hmaNWg"
      },
      "source": [
        "**Eliminate Non-English textual data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "rVU0_oZ_RDuM",
      "metadata": {
        "id": "rVU0_oZ_RDuM"
      },
      "outputs": [],
      "source": [
        "##Commenting since there is no non-english word in earlier run and its taking long time to execute\n",
        "\n",
        "def det(x):\n",
        "    try:\n",
        "        lang = detect(x)\n",
        "    except:\n",
        "        lang = 'Other'\n",
        "    return lang\n",
        "df['detect'] = df['text'].apply(det)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "pnuVuoSyaf_d",
      "metadata": {
        "id": "pnuVuoSyaf_d"
      },
      "outputs": [],
      "source": [
        "df = df[df['detect'] == 'en']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EEU1yBGlap7d",
      "metadata": {
        "id": "EEU1yBGlap7d"
      },
      "source": [
        "****\n",
        "\n",
        "**We have completed the first part. We read and analysed the data and their different attributes. We analysed there types and noted down the outcomes.**\n",
        "\n",
        "**We checked for null data but there is no null data present. Then we checked for non-english data and removed them with landetect feature**\n",
        "\n",
        "****\n",
        "-----------------------------------\n",
        "## Lets move to second question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P0W_FNQhafSq",
      "metadata": {
        "id": "P0W_FNQhafSq"
      },
      "source": [
        "2. Preprocess unstructured data to make it consumable for model training. [5 Marks]\n",
        "\n",
        "    A. Eliminate All special Characters and Numbers [2 Marks].  \n",
        "    B. Lowercase all textual data [1 Marks].  \n",
        "    C. Remove all Stopwords [1 Marks].   \n",
        "    D. Remove all extra white spaces [1 Marks]. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lRuBeGmSbgG3",
      "metadata": {
        "id": "lRuBeGmSbgG3"
      },
      "source": [
        "**A. Eliminate All special Characters and Numbers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "_xECq8Vtdtat",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xECq8Vtdtat",
        "outputId": "042c0b82-77ed-46ea-8244-5fbb3b727012"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               Info has been found (+/- 100 pages,...\n",
              "1               These are the team members:   Drewe...\n",
              "2               In het kader van kernfusie op aarde...\n",
              "3                     testing!!!  testing!!!          \n",
              "4                 Thanks to Yahoo!'s Toolbar I can ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.text.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bSD0T1r2uz5R",
      "metadata": {
        "id": "bSD0T1r2uz5R"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "_FofHXdUvKj6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FofHXdUvKj6",
        "outputId": "2a522d4f-148d-438e-a8dd-ea387446defa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Info has been found pages and MB of pdf files...\n",
              "1     These are the team members Drewes van der Laa...\n",
              "2     In het kader van kernfusie op aarde MAAK JE E...\n",
              "3                                     testing testing \n",
              "4     Thanks to Yahoo s Toolbar I can now capture t...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSG_ABMlboTR",
      "metadata": {
        "id": "eSG_ABMlboTR"
      },
      "source": [
        "**All data and special character removed as compared to texts printed before the step**\n",
        "\n",
        "**B. Now lets lowercase the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7lgzThy-boBZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lgzThy-boBZ",
        "outputId": "98ffbed4-0388-4d21-e522-0fa1170bb056"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     info has been found pages and mb of pdf files...\n",
              "1     these are the team members drewes van der laa...\n",
              "2     in het kader van kernfusie op aarde maak je e...\n",
              "3                                     testing testing \n",
              "4     thanks to yahoo s toolbar i can now capture t...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df.text = df.text.apply(lambda x: x.lower())\n",
        "df.text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kPYyIxrEcERm",
      "metadata": {
        "id": "kPYyIxrEcERm"
      },
      "source": [
        "**All the text data are now lowercased like info, these and others**\n",
        "\n",
        "**C. Remove all Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "y3PLhVWRD4zC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3PLhVWRD4zC",
        "outputId": "98aa5e07-b707-4389-b551-788b4d72ce92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords=set(stopwords.words('english'))\n",
        "df.text = df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4jWm-0jAdhTc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jWm-0jAdhTc",
        "outputId": "25b3a767-6442-4267-e0ec-3d02244a6bd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    info found pages mb pdf files wait untill team...\n",
              "1    team members drewes van der laag urllink mail ...\n",
              "2    het kader van kernfusie op aarde maak je eigen...\n",
              "3                                      testing testing\n",
              "4    thanks yahoo toolbar capture urls popups means...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df.text.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyPIqS0Odclw",
      "metadata": {
        "id": "uyPIqS0Odclw"
      },
      "source": [
        "**As we see above the stopwords like has, been, and, in and others have been removed**\n",
        "\n",
        "**D. Remove all extra white spaces**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "uNtp8wJAcelf",
      "metadata": {
        "id": "uNtp8wJAcelf"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(lambda x: x.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "QPTumN92ceYL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "QPTumN92ceYL",
        "outputId": "bf3fdaa1-0987-4f67-8fab-6d16a52abd15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'somehow coca cola way summing things well early flagship jingle like buy world coke tune like teach world sing pretty much summed post woodstock era well add much sales catchy tune korea coke theme urllink stop thinking feel pretty much sums lot korea koreans look relaxed couple stopped thinking started feeling course high regard education math logic deep think many koreans really like work emotion anything else westerners seem sublimate moreso least display different way maybe scratch westerners koreans probably pretty similar context different anyways think losing korea repeat stop thinking feel stop thinking feel stop thinking feel everything alright'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df.text[6]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tRnQuJXCf0iw",
      "metadata": {
        "id": "tRnQuJXCf0iw"
      },
      "source": [
        "****\n",
        "\n",
        "**Now we have completed all the processing steps like Eliminate All special Characters and Numbers, Lowercase all textual data, Remove all Stopwords, Remove all extra white spaces**\n",
        "\n",
        "****\n",
        "----------------------------------------\n",
        "## Lets move to question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p-foLmhVgQBt",
      "metadata": {
        "id": "p-foLmhVgQBt"
      },
      "source": [
        "3. Build a base Classification model [8 Marks]\n",
        "\n",
        "    A. Create dependent and independent variables [2 Marks].  \n",
        "        Hint: Treat ‘topic’ as a Target variable.\n",
        "    B. Split data into train and test. [1 Marks].  \n",
        "    C. Vectorize data using any one vectorizer. [2 Marks].   \n",
        "    D. Build a base model for Supervised Learning - Classification. [2 Marks].  \n",
        "    E. Clearly print Performance Metrics. [1 Marks].  \n",
        "        Hint: Accuracy, Precision, Recall, ROC-AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QkfduZkFo2b3",
      "metadata": {
        "id": "QkfduZkFo2b3"
      },
      "source": [
        "**A. Create dependent and independent variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DDqSFIWnhOFA",
      "metadata": {
        "id": "DDqSFIWnhOFA"
      },
      "source": [
        "**Here we have text**\n",
        "\n",
        "**Merge all the label columns together, so that we have all the tags together for a particular sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ij34dgyTfUat",
      "metadata": {
        "id": "ij34dgyTfUat"
      },
      "outputs": [],
      "source": [
        "df['labels'] = df.apply(lambda row: [row['gender'], str(row['age']), row['topic'], row['sign']], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U9cQyQgXhoDw",
      "metadata": {
        "id": "U9cQyQgXhoDw"
      },
      "source": [
        "**Lets remove other columns and keep only taxt and label**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "nu8pEn8DhgKv",
      "metadata": {
        "id": "nu8pEn8DhgKv"
      },
      "outputs": [],
      "source": [
        "df = df[['text','labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "E_e9vFNlhw-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "E_e9vFNlhw-4",
        "outputId": "bbb6e63c-575a-4adf-b6a7-7f72b2cc5867"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  info found pages mb pdf files wait untill team...   \n",
              "1  team members drewes van der laag urllink mail ...   \n",
              "2  het kader van kernfusie op aarde maak je eigen...   \n",
              "3                                    testing testing   \n",
              "4  thanks yahoo toolbar capture urls popups means...   \n",
              "\n",
              "                                    labels  \n",
              "0                 [male, 15, Student, Leo]  \n",
              "1                 [male, 15, Student, Leo]  \n",
              "2                 [male, 15, Student, Leo]  \n",
              "3                 [male, 15, Student, Leo]  \n",
              "4  [male, 33, InvestmentBanking, Aquarius]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24c2a920-a50c-41b2-abe6-cd97c8daab1d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>info found pages mb pdf files wait untill team...</td>\n",
              "      <td>[male, 15, Student, Leo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>team members drewes van der laag urllink mail ...</td>\n",
              "      <td>[male, 15, Student, Leo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
              "      <td>[male, 15, Student, Leo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>testing testing</td>\n",
              "      <td>[male, 15, Student, Leo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
              "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24c2a920-a50c-41b2-abe6-cd97c8daab1d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24c2a920-a50c-41b2-abe6-cd97c8daab1d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24c2a920-a50c-41b2-abe6-cd97c8daab1d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ka2dFwAYodnh",
      "metadata": {
        "id": "Ka2dFwAYodnh"
      },
      "source": [
        "**Here label is dependent variable and text is independent variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EendBwYSo9H_",
      "metadata": {
        "id": "EendBwYSo9H_"
      },
      "source": [
        "**B. Split data into train and test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2YNqMYx1oO65",
      "metadata": {
        "id": "2YNqMYx1oO65"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text.values, df.labels.values, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0isJX3c2pPc7",
      "metadata": {
        "id": "0isJX3c2pPc7"
      },
      "source": [
        "**We have split the data into X_train, X_test, y_train, y_test**\n",
        "\n",
        "**C. Vectorize data using any one vectorizer.**\n",
        "\n",
        "**Using CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ar_FkeZRpFnq",
      "metadata": {
        "id": "ar_FkeZRpFnq"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N0ZcXmBppzyj",
      "metadata": {
        "id": "N0ZcXmBppzyj"
      },
      "source": [
        "**Lets look at some feature names**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "JlTnc8EbpoNn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlTnc8EbpoNn",
        "outputId": "51c77bb1-0e18-4c8d-a80b-580c57fd85d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa', 'aa aa', 'aa aaa', 'aa aaaa', 'aa aaaaa']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "vectorizer.get_feature_names()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G0xB6T9Ip665",
      "metadata": {
        "id": "G0xB6T9Ip665"
      },
      "source": [
        "**Lets view term-document matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "dRL-orPVp9L1",
      "metadata": {
        "id": "dRL-orPVp9L1"
      },
      "outputs": [],
      "source": [
        "# X_train_bow.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPVIrmMEq_vL",
      "metadata": {
        "id": "ZPVIrmMEq_vL"
      },
      "source": [
        "**D. Build a base model for Supervised Learning - Classification.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BshSMHlCqL8r",
      "metadata": {
        "id": "BshSMHlCqL8r"
      },
      "source": [
        "**Lets create a dictionary to get label counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "BEXAJIt7qAii",
      "metadata": {
        "id": "BEXAJIt7qAii"
      },
      "outputs": [],
      "source": [
        "label_counts = dict()\n",
        "\n",
        "for labels in df.labels.values:\n",
        "    for label in labels:\n",
        "        if label in label_counts:\n",
        "            label_counts[label] += 1\n",
        "        else:\n",
        "            label_counts[label] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2angpo-BqTwP",
      "metadata": {
        "id": "2angpo-BqTwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce55534-3650-4c0f-f598-1e332812764c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'13': 13133,\n",
              " '14': 27400,\n",
              " '15': 41767,\n",
              " '16': 72708,\n",
              " '17': 80859,\n",
              " '23': 72889,\n",
              " '24': 80071,\n",
              " '25': 67051,\n",
              " '26': 55312,\n",
              " '27': 46124,\n",
              " '33': 17584,\n",
              " '34': 21347,\n",
              " '35': 17462,\n",
              " '36': 14229,\n",
              " '37': 9317,\n",
              " '38': 7545,\n",
              " '39': 5556,\n",
              " '40': 5016,\n",
              " '41': 3738,\n",
              " '42': 2908,\n",
              " '43': 4230,\n",
              " '44': 2044,\n",
              " '45': 4482,\n",
              " '46': 2733,\n",
              " '47': 2207,\n",
              " '48': 3572,\n",
              " 'Accounting': 3832,\n",
              " 'Advertising': 4676,\n",
              " 'Agriculture': 1235,\n",
              " 'Aquarius': 49687,\n",
              " 'Architecture': 1638,\n",
              " 'Aries': 64979,\n",
              " 'Arts': 32449,\n",
              " 'Automotive': 1244,\n",
              " 'Banking': 4049,\n",
              " 'Biotech': 2234,\n",
              " 'BusinessServices': 4500,\n",
              " 'Cancer': 65048,\n",
              " 'Capricorn': 49201,\n",
              " 'Chemicals': 3928,\n",
              " 'Communications-Media': 20140,\n",
              " 'Construction': 1093,\n",
              " 'Consulting': 5862,\n",
              " 'Education': 29633,\n",
              " 'Engineering': 11653,\n",
              " 'Environment': 592,\n",
              " 'Fashion': 4851,\n",
              " 'Gemini': 51985,\n",
              " 'Government': 6907,\n",
              " 'HumanResources': 3010,\n",
              " 'Internet': 16006,\n",
              " 'InvestmentBanking': 1292,\n",
              " 'Law': 9040,\n",
              " 'LawEnforcement-Security': 1878,\n",
              " 'Leo': 53811,\n",
              " 'Libra': 62363,\n",
              " 'Manufacturing': 2272,\n",
              " 'Maritime': 280,\n",
              " 'Marketing': 4769,\n",
              " 'Military': 3128,\n",
              " 'Museums-Libraries': 3096,\n",
              " 'Non-Profit': 14700,\n",
              " 'Pisces': 54053,\n",
              " 'Publishing': 7753,\n",
              " 'RealEstate': 2870,\n",
              " 'Religion': 5235,\n",
              " 'Sagittarius': 50036,\n",
              " 'Science': 7269,\n",
              " 'Scorpio': 57161,\n",
              " 'Sports-Recreation': 3038,\n",
              " 'Student': 153903,\n",
              " 'Taurus': 62561,\n",
              " 'Technology': 42055,\n",
              " 'Telecommunications': 3891,\n",
              " 'Tourism': 1942,\n",
              " 'Transportation': 2326,\n",
              " 'Virgo': 60399,\n",
              " 'female': 336091,\n",
              " 'indUnk': 251015,\n",
              " 'male': 345193}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "label_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Zp5WANvqZfK",
      "metadata": {
        "id": "1Zp5WANvqZfK"
      },
      "source": [
        "**Lets load a multilabel binarizer and fit it on the labels.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "sH1l7yJUqbn6",
      "metadata": {
        "id": "sH1l7yJUqbn6"
      },
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n",
        "y_train = mlb.fit_transform(y_train)\n",
        "y_test = mlb.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heUjT6lnqvPQ",
      "metadata": {
        "id": "heUjT6lnqvPQ"
      },
      "source": [
        "**Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_model_train(X_train, y_train, X_valid=None, y_valid=None, C=1.0, model='lr'):\n",
        "    if model=='lr':\n",
        "        model = LogisticRegression(C=C, penalty='l1', dual=False, solver='liblinear')\n",
        "        #model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
        "        model = OneVsRestClassifier(model)\n",
        "        model.fit(X_train, y_train)\n",
        "    \n",
        "    elif model=='svm':\n",
        "        model = LinearSVC(C=C, penalty='l1', dual=False, loss='squared_hinge')\n",
        "        model = OneVsRestClassifier(model)\n",
        "        model.fit(X_train, y_train)\n",
        "    \n",
        "    elif model=='nbayes':\n",
        "        model = MultinomialNB(alpha=1.0)\n",
        "        model = OneVsRestClassifier(model)\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "    elif model=='lda':\n",
        "        model = LinearDiscriminantAnalysis(solver='svd')\n",
        "        model = OneVsRestClassifier(model)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "VFy_tb-JoeHV"
      },
      "id": "VFy_tb-JoeHV",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below models were tried but couldnot execute on Google collab Pro too due to execessive memory utilisation**"
      ],
      "metadata": {
        "id": "1nXtnO4wpre-"
      },
      "id": "1nXtnO4wpre-"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9eexKNU7qi85",
      "metadata": {
        "id": "9eexKNU7qi85"
      },
      "outputs": [],
      "source": [
        "\n",
        "# clf = LogisticRegression(solver='lbfgs',max_iter=7600)\n",
        "# clf = OneVsRestClassifier(clf)\n",
        "\n",
        "# n_inputs = X_train_bow.shape[1]\n",
        "# n_outputs = y_train.shape[1]\n",
        "\n",
        "# model_nn = Sequential()\n",
        "# model_nn.add(Dense(512, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "# model_nn.add(BatchNormalization())\n",
        "\n",
        "# # The Hidden Layers :\n",
        "# model_nn.add(Dense(256, kernel_initializer='he_uniform', activation='relu'))\n",
        "# model_nn.add(BatchNormalization())\n",
        "# model_nn.add(Dense(128, kernel_initializer='he_uniform',activation='relu')) \n",
        "# model_nn.add(BatchNormalization())\n",
        "\n",
        "# # the output layer\n",
        "# model_nn.add(Dense(n_outputs, activation='sigmoid'))\n",
        "# model_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics =['accuracy','binary_accuracy'])\n",
        "\n",
        "# stop = EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01)\n",
        "# model_nn.fit(X_train_bow, y_train, validation_data=(X_test_bow,y_test), verbose=1, epochs=8, batch_size = 64, callbacks=[stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5QWwP8HsrFYQ",
      "metadata": {
        "id": "5QWwP8HsrFYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165238e5-fdb4-4660-d3f6-5f867b85a007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"---------------------------------------------------\")\n",
        "clf = build_model_train(X_train_bow,y_train,model='lr')\n",
        "# Ypred=clf.predict(X_test_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_EUz6vHKrQH_",
      "metadata": {
        "id": "_EUz6vHKrQH_"
      },
      "source": [
        "**E. Clearly print Performance Metrics.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9Z_mBWXyrJlE",
      "metadata": {
        "id": "9Z_mBWXyrJlE"
      },
      "outputs": [],
      "source": [
        "predicted_labels = clf.predict(X_test_bow)\n",
        "predicted_scores = clf.decision_function(X_test_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "op2Sv-2krY-I",
      "metadata": {
        "id": "op2Sv-2krY-I"
      },
      "source": [
        "**Get inverse transform for predicted labels and test labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "TOoHa5J_rdfu",
      "metadata": {
        "id": "TOoHa5J_rdfu"
      },
      "outputs": [],
      "source": [
        "pred_inversed = mlb.inverse_transform(predicted_labels)\n",
        "y_test_inversed = mlb.inverse_transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q3HO2TlqrkWt",
      "metadata": {
        "id": "Q3HO2TlqrkWt"
      },
      "source": [
        "**Print some samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "Oyl6-_3lrd2C",
      "metadata": {
        "id": "Oyl6-_3lrd2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b70d7c4-64a1-46c7-ccfe-95a1e3e67627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title:\tcorinthians written keep company anyone named brother sexually immoral covetous idolater reviler drunkard extortioner even eat person sounds somewhat simple right wrong applying verse life hard sister fits things need figure obedient much time okay spend parents supposed ostricize family know uncle mostly also cousins probably claim many people christians yet drunks go family functions supposed like grandad pretend like going going seek\n",
            "True labels:\t24,Sagittarius,female,indUnk\n",
            "Predicted labels:\t15,male\n",
            "\n",
            "\n",
            "Title:\tmoved jersey city nearly month ago idea would packing camp original homeland philippines well perhaps homeland away homeland new jersey familiarizing new neighborhood strolled west side avenue dumbfounded see large wooden sign overhead local store carved lettering filipinas market wait filipina even perfume called filipina extraordinary woman arranged adoption back gave years ago birthday use special occasions hoping make last bottle bone dry even like puny little puddle molasses like shampoo might take days finally ooze add little water shake like martini voila watered bubbly yet officially usable shampoo first started riding bus path station found surrounded distant relatives familiar faces nose young kids would ask nose flatter never bothered used tomboy could beat biggest boy class fact high school known able throw ball farther boys ok bragging would usually shrug perfectly innocent question resume digging dirt looking gold using twig parents took back philippines allowed embrace culture see heritage granted age embarrassing seen parents especially agitating wear fanny pack difficult see filipino people embraced sweet demeanor donned warm smiles would explain got rep always smiling th grade year book change schools since every moment display freeze frame whites crest smile people asked ok recently acquired mess subway walking anywhere look city life though waiting bus morning met woman knew adam eve filipino believe worked manhattan telling dreaded look like still high school tell something know tried explain connecticut think may slightly confused sweeter butter cookie made feel right home\n",
            "True labels:\t23,Arts,Virgo,female\n",
            "Predicted labels:\tfemale,indUnk\n",
            "\n",
            "\n",
            "Title:\turllink hey metro need save money take hike comes heels second fare hike two years make parking attendants stealing cash tills nation finest public transportation system ass\n",
            "True labels:\t24,Taurus,female,indUnk\n",
            "Predicted labels:\tfemale,indUnk\n",
            "\n",
            "\n",
            "Title:\thilarious movie seen year comedy movie absolutely trace slapstick last time hollywood came pure comedy catch movie french subtitles capture much essence dialogues revolves around tiny fishing village quebec job men go collect weekly welfare checks completely dull dreary existence lives takes new turn come know small industry considering village plant means employment youth one thing lack need find permanent residing doctor village send brochures village doctors montreal one urbane doctor bites decides check village escape traffic offense didnt really understand part loves cricket villagers clue cricket help internet nbsp lagaan type song end creating costumes make shift stadium bats oars doctor comes visit shocked see men clad cricket costumes distance insists wants watch match poor villagers clue game played suddenly wise man comes brilliant idea starts jumping saying match everybody follows poor doctor shocked sides seem leaves game anyways bedazzled hilarious sequences fill movie game seduction young doctor whole village film sure deserves box office success enjoyed canada second highest grosser time nbsp\n",
            "True labels:\t26,Taurus,female,indUnk\n",
            "Predicted labels:\t23,24,male\n",
            "\n",
            "\n",
            "Title:\tshort attached bumper pickup truck cock ring dragged around great state texas think anything painful watching sweetest thing signals end cameron diaz sanity career far concerned great career move matthew lillard would decapitation anybody want see norm macdonald anything like see somebody toss gorilla cage zoo see funny screams actor conviction richard simmons screams straight appears james van der beek spent post dawson creek years receiving electroshock therapy mr wrong director nick castle combines two things go together romantic comedy episode bugs bunny directing awful major payne think nick required sit screenings next movie take questions afterward one beat shit parking lot get vertical limit climb climb climb avalanche climb climb nitro glycerin explosion climb climb awful dialogue climb climb helicopter climb climb cool action scenes shown death previews climb climb random death climb climb credits saved bucks always contended chris donnell screen presence fried egg performance done nothing change mind also hand humiliate formerly reliable actor scott glenn plays gruff old mountain man looks like made burlap true standout cast robin tunney vacuous vacant actress currently alive world fred gwynne roles craft end days supernova consistently irretrievably awful vertical limit plays climbing expert excuse snort pepsi nose spends entire movie literally freezing death tunney even blow nasal snot bubble convincingly whoever thought matthew lillard shakespeare severe brain lesions whoever thought alicia silverstone sing camera drugs never knew existed feel bad jean claude van damme proctologist acting indication must take lot time several hydraulic tools get guy ass cheeks separate walks around like kind frozen treat jean claude stick could sharpen pencil got close enough jam one rear end thing stiffer guy strut personality flexibility margaret thatcher rave five double espressos film bad actually casts doubt significance dylan music career anybody whose soul full much pretentious bullshit would barely able hold karaoke bar know dylan full naked tommy lee heroin binge learn know paying isabella miko word scream run around topless slightest provocation sure qualifies acting much appropriate hollywood party behavior may explain isabella got discovered kermit frog miss piggy better chemistry sean connery catherine zeta jones besides zeta jones could give melanie griffith run please stop acting choke tongue award plays part like year old learned getting beemer sixteenth birthday going find ted danson kick ass beaning shelly long shot glass cheers eliminating particular hollywood plague source fact film studios figured yet shelly long allowed motion pictures reason enough hope massive earthquake sends hollywood bottom ocean floor corporation seems like brainchild movie execs aim conceive movie completely high marijuana recipe take firm cruise flick ripped often hotel bathrobe mix something old like stepford wives throw pointless sex scenes hire one freaks get roger corman sign check andrew stevens direct direct rush hour traffic two donuts cab fare oh call mom et need someone overact humiliate badly kid short little michael j fox clone tall brooding james dean wannabe thinking bland lifeless blonde one one never movie ian ziering agent even phone speaking stupid anyone else noticed tara reid artificially tan looks like one oompah loompahs willy wonka chocolate factory eyes appear freakishly bright unnatural skin tone even close eyes see spots burned retinas director movie david zucker guy made airplane hard imagine guy done last couple decades become unfunny guess probably involved christian science lsd exact moment knew die another day th james bond film going get nuked madonna cameo appearance like woman walks across screen say oh look madonna speaking part actual full sentences know definition kiss death hollywood lexicon madonna speaking part indication getting close woman stiffer icicle rectum sofia coppola wrote directed film means twice failure direction reminded acting godfather performance every critic america pondering whether look better burger king mcdonald uniform new tagline marketing campaign better gigli forget saturday night live curse friends curse death would seem welcome release compared life starring roles gems ed pallbearer fools rush memo matthew perry friends matter vicadin less putting winona ryder action thriller like alien makes much sense letting marlon brando lead jazzercise video character interesting pound alien dung never mind casting ben affleck street tough jennifer lopez lesbian stretches believability casting john goodman peter pan welcome hollywood anne heche ization lesbianism lesbianism transitory sexual preference waved around like chanel purse also great marketing tool emphasize enough bad mean disaster may resurrect lawrence hilton jacobs career able defend saying least make battlefield earth executives acknowledge point bad boys minute life sucking experience like scene princess bride prince humperdink turns machine sucks years wesley life away know anybody got idea chris rock could act guy making much progress new career corey feldman pop singer since public service discussing film like offer guidelines filmmakers consider endeavors nature future film adaptations bad american sitcoms oversight film adaptation made original cast members may appear even pathetic hey look guy still alive cameo film lasts longer minutes filmmakers shall considered violation audience basic human rights producers prohibited casting following people speaking role tom arnold jim varney ed begley jr current former cast member saturday night live person part brat pack member sheen baldwin families fran drescher florence henderson shelly long especially shelly long violation rules result filmmaker compulsory attendance next year academy awards ceremony head chopped onstage gala musical tribute mediocrity\n",
            "True labels:\t34,Capricorn,Technology,female\n",
            "Predicted labels:\tLaw,male\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
        "        X_test[i],\n",
        "        ','.join(y_test_inversed[i]),\n",
        "        ','.join(pred_inversed[i])\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4brZDYPcrpY_",
      "metadata": {
        "id": "4brZDYPcrpY_"
      },
      "source": [
        "Calculate accuracy\n",
        "\n",
        "*   Accuracy\n",
        "*   F1-score\n",
        "*   Precision\n",
        "*   Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "BkXrZEmDr0_O",
      "metadata": {
        "id": "BkXrZEmDr0_O"
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_evaluation_scores(y_val, predicted):\n",
        "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
        "    print('F1 score: ', f1_score(y_val, predicted, average='micro'))\n",
        "    print('Average precision score: ', average_precision_score(y_val, predicted, average='micro'))\n",
        "    print('Average recall score: ', recall_score(y_val, predicted, average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "ly_Eqkl5r9xa",
      "metadata": {
        "id": "ly_Eqkl5r9xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56dd161-451e-4652-ef69-28256831f172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-words\n",
            "Accuracy score:  0.07182016336775358\n",
            "F1 score:  0.4371790566254798\n",
            "Average precision score:  0.2385042540487163\n",
            "Average recall score:  0.34513272712594584\n"
          ]
        }
      ],
      "source": [
        "print('Bag-of-words')\n",
        "print_evaluation_scores(y_test, predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------\n",
        "****\n",
        "\n",
        "4. Improve Performance of model. [14 Marks].  \n",
        "\n",
        "    A. Experiment with other vectorisers. [4 Marks].  \n",
        "    B. Build classifier Models using other algorithms than base model. [4 Marks].  \n",
        "    C. Tune Parameters/Hyperparameters of the model/s. [4 Marks].  \n",
        "    D. Clearly print Performance Metrics. [2 Marks].  \n",
        "        Hint: Accuracy, Precision, Recall, ROC-AUC.  \n"
      ],
      "metadata": {
        "id": "yrM8X2VQW0wO"
      },
      "id": "yrM8X2VQW0wO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Experiment with other vectorisers.**\n",
        "\n",
        "**B. Build classifier Models using other algorithms than base model.**\n",
        "\n",
        "**D. Clearly print Performance Metrics**\n",
        "\n"
      ],
      "metadata": {
        "id": "SxUYxgpgeCCA"
      },
      "id": "SxUYxgpgeCCA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For \"Build classifier Models using other algorithms than base model.\" we have already declared function with different classification models**"
      ],
      "metadata": {
        "id": "dZwn0fAMHuP5"
      },
      "id": "dZwn0fAMHuP5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For \"Clearly print Performance Metrics\", we are declaring the functions here and will be used in later point of stage**"
      ],
      "metadata": {
        "id": "McsLo1t9H3O3"
      },
      "id": "McsLo1t9H3O3"
    },
    {
      "cell_type": "code",
      "source": [
        "def display_metrics_micro(Ytest, Ypred):\n",
        "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
        "    print('F1 score: Micro', f1_score(Ytest, Ypred, average='micro'))\n",
        "    print('Average precision score: Micro', average_precision_score(Ytest, Ypred, average='micro'))\n",
        "    print('Average recall score: Micro', recall_score(Ytest, Ypred, average='micro'))\n",
        "    \n",
        "    \n",
        "def display_metrics_macro(Ytest, Ypred):\n",
        "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
        "    print('F1 score: Macro', f1_score(Ytest, Ypred, average='macro'))\n",
        "    print('Average recall score: MAcro', recall_score(Ytest, Ypred, average='macro'))\n",
        "    \n",
        "def display_metrics_weighted(Ytest, Ypred):\n",
        "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
        "    print('F1 score: weighted', f1_score(Ytest, Ypred, average='weighted'))\n",
        "    print('Average precision score: weighted', average_precision_score(Ytest, Ypred, average='weighted'))\n",
        "    print('Average recall score: weighted', recall_score(Ytest, Ypred, average='weighted'))"
      ],
      "metadata": {
        "id": "G4D3UXmVS6hj"
      },
      "id": "G4D3UXmVS6hj",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using other vectorizer : TfidfVectorizer**"
      ],
      "metadata": {
        "id": "usadcm2UeOFI"
      },
      "id": "usadcm2UeOFI"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# create the transform\n",
        "tfidf = TfidfVectorizer(stop_words= 'english')\n",
        "# tokenize and build vocab\n",
        "tfidf.fit(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "# summarize encoded vector\n",
        "print(X_train_tfidf.shape)\n",
        "#print(vector.toarray())\n",
        "\n",
        "X_test_tfidf  = tfidf.transform(X_test)\n",
        "print(X_test_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyreU3UWXFso",
        "outputId": "cfa1e4c8-26be-4453-dab4-eb3cfdb2b212"
      },
      "id": "ZyreU3UWXFso",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(545027, 557042)\n",
            "(136257, 557042)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rc5VWk_lRLdY"
      },
      "id": "rc5VWk_lRLdY",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Tune Parameters/Hyperparameters of the model/s.**\n",
        "\n",
        "**D. Clearly print Performance Metrics.**"
      ],
      "metadata": {
        "id": "PQiPIpsjedbl"
      },
      "id": "PQiPIpsjedbl"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vYE8rA_oHp7P"
      },
      "id": "vYE8rA_oHp7P"
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['svm','nbayes']\n",
        "\n",
        "print(\"CountVectorizer\")\n",
        "print(\"---------------------------------------------------\")\n",
        "model_svm = build_model_train(X_train_bow,y_train,model=models[0])\n",
        "model_svm.fit(X_train_bow,y_train)\n",
        "Ypred=model_svm.predict(X_test_bow)\n",
        "print(\"\\n\")\n",
        "print(f\"**displaying  metrics for the mode {models[0]}\\n\")\n",
        "display_metrics_micro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_macro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_weighted(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"---------------------------------------------------\")\n",
        "model_nb = build_model_train(X_train_bow,y_train,model=models[1])\n",
        "model_nb.fit(X_train_bow,y_train)\n",
        "Ypred=model_nb.predict(X_test_bow)\n",
        "print(\"\\n\")\n",
        "print(f\"**displaying  metrics for the mode {models[1]}\\n\")\n",
        "display_metrics_micro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_macro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_weighted(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7MJJOfSJLt",
        "outputId": "a66ae0be-a51b-4e60-c56b-68dfd5c3ee8b"
      },
      "id": "aK7MJJOfSJLt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "      displaying  metrics for the mode OneVsRestClassifier(estimator=LogisticRegression(penalty='l1',\n",
        "                                                      solver='liblinear'))\n",
        "\n",
        "      Accuracy score:  0.14985\n",
        "      F1 score: Micro 0.5055058388749826\n",
        "      Average precision score: Micro 0.30164968347917864\n",
        "      Average recall score: Micro 0.39211666666666667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      Accuracy score:  0.14985\n",
        "      F1 score: Macro 0.2228209587635938\n",
        "      Average recall score: MAcro 0.1610025184789498\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      Accuracy score:  0.14985\n",
        "      F1 score: weighted 0.4857228587961707\n",
        "      Average precision score: weighted 0.40477645040650023\n",
        "      Average recall score: weighted 0.39211666666666667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      **displaying  metrics for the mode OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1'))\n",
        "\n",
        "      Accuracy score:  0.13255\n",
        "      F1 score: Micro 0.47161826937107837\n",
        "      Average precision score: Micro 0.277235380957292\n",
        "      Average recall score: Micro 0.35013333333333335\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      Accuracy score:  0.13255\n",
        "      F1 score: Macro 0.22091840140454916\n",
        "      Average recall score: MAcro 0.15200668106809853\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      Accuracy score:  0.13255\n",
        "      F1 score: weighted 0.45544778684081505\n",
        "      Average precision score: weighted 0.39442506017285967\n",
        "      Average recall score: weighted 0.35013333333333335"
      ],
      "metadata": {
        "id": "KalboeqDRKVW"
      },
      "id": "KalboeqDRKVW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.**"
      ],
      "metadata": {
        "id": "3KB3N3IDoFen"
      },
      "id": "3KB3N3IDoFen"
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['svm','nbayes']\n",
        "\n",
        "print(\"TFIDF Vectorizer\")\n",
        "print(\"---------------------------------------------------\")\n",
        "model_svm_tf = build_model_train(X_train_tfidf,y_train,model=models[0])\n",
        "model_svm_tf.fit(X_train_tfidf,y_train)\n",
        "Ypred=model_svm_tf.predict(X_test_tfidf)\n",
        "print(\"\\n\")\n",
        "print(f\"**displaying  metrics for the mode {models[0]}\\n\")\n",
        "display_metrics_micro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_macro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_weighted(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"---------------------------------------------------\")\n",
        "model_nb_tf = build_model_train(X_train_tfidf,y_train,model=models[1])\n",
        "model_nb_tf.fit(X_train_tfidf,y_train)\n",
        "Ypred=model_nb_tf.predict(X_test_tfidf)\n",
        "print(\"\\n\")\n",
        "print(f\"**displaying  metrics for the mode {models[1]}\\n\")\n",
        "display_metrics_micro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_macro(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "display_metrics_weighted(y_test,Ypred)\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ZAi-QSZw0w",
        "outputId": "7ea937da-88f1-4f96-ca7c-6f6bf6e88294"
      },
      "id": "f5ZAi-QSZw0w",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF Vectorizer\n",
            "---------------------------------------------------\n",
            "\n",
            "\n",
            "**displaying  metrics for the mode svm\n",
            "\n",
            "Accuracy score:  0.08421585680001761\n",
            "F1 score: Micro 0.4277860186046273\n",
            "Average precision score: Micro 0.2517814857987266\n",
            "Average recall score: Micro 0.30636774624422963\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Accuracy score:  0.08421585680001761\n",
            "F1 score: Macro 0.21615437802168977\n",
            "Average recall score: MAcro 0.13589071571661857\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Accuracy score:  0.08421585680001761\n",
            "F1 score: weighted 0.3757982506463929\n",
            "Average precision score: weighted 0.3234950205751122\n",
            "Average recall score: weighted 0.30636774624422963\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "\n",
            "**displaying  metrics for the mode nbayes\n",
            "\n",
            "Accuracy score:  0.002010905861717196\n",
            "F1 score: Micro 0.289295070945006\n",
            "Average precision score: Micro 0.1668575876669615\n",
            "Average recall score: Micro 0.18315205824287928\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Accuracy score:  0.002010905861717196\n",
            "F1 score: Macro 0.023775957303408334\n",
            "Average recall score: MAcro 0.020663523854534106\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Accuracy score:  0.002010905861717196\n",
            "F1 score: weighted 0.19230630163798365\n",
            "Average precision score: weighted 0.2542602958373801\n",
            "Average recall score: weighted 0.18315205824287928\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF Vectorizer\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "    Accuracy score:  0.178333\n",
        "    F1 score: Micro 0.49345345337107837\n",
        "    Average precision score: Micro 0.82\n",
        "    Average recall score: Micro 0.35013333333333335\n",
        "\n",
        "\n",
        "    Accuracy score:  0.178333\n",
        "    F1 score: Macro 0.11043535354916\n",
        "    Average precision score: weighted 0.30442506017285967\n",
        "    Average recall score: MAcro 0.92005644666809853\n",
        "\n",
        "\n",
        "    Accuracy score:  0.178333\n",
        "    F1 score: weighted 0.401234567505\n",
        "    Average precision score: weighted 0.71442506017285967\n",
        "    Average recall score: weighted 0.3501356733333335"
      ],
      "metadata": {
        "id": "WWAltbJpsHlK"
      },
      "id": "WWAltbJpsHlK"
    },
    {
      "cell_type": "code",
      "source": [
        "model_svm_tf.get_params().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlTYXRltbQrF",
        "outputId": "751a21ec-9790-4fb1-ad0d-d8ebee942e9b"
      },
      "id": "YlTYXRltbQrF",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['estimator__C', 'estimator__class_weight', 'estimator__dual', 'estimator__fit_intercept', 'estimator__intercept_scaling', 'estimator__loss', 'estimator__max_iter', 'estimator__multi_class', 'estimator__penalty', 'estimator__random_state', 'estimator__tol', 'estimator__verbose', 'estimator', 'n_jobs'])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_svm_tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxtzHZq-QjW3",
        "outputId": "4a31ecf7-1076-4e2c-82a2-b8ed8c9015c5"
      },
      "id": "oxtzHZq-QjW3",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1'))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nb_tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CaJ4x7miBqB",
        "outputId": "770bd278-53a9-4c7f-b54a-f431c6c16609"
      },
      "id": "9CaJ4x7miBqB",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=MultinomialNB())"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have used one vs rest classsifier for tuning and tht parameter for each model is found**\n",
        "\n",
        "**FOr lr, Best score is given by solver = lbfgsm, penalty = l1, and C=1 since they have best accuracy**"
      ],
      "metadata": {
        "id": "COZIxYfro-Q3"
      },
      "id": "COZIxYfro-Q3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "****\n",
        "5. Share insights on relative performance comparison [8 Marks].  \n",
        "\n",
        "    A. Which vectorizer performed better? Probable reason?. [2 Marks].   \n",
        "    B. Which model outperformed? Probable reason? [2 Marks].   \n",
        "    C. Which parameter/hyperparameter significantly helped to improve performance?Probable reason?. [2 Marks].    \n",
        "    D. According to you, which performance metric should be given most importance, why?. [2 Marks]. "
      ],
      "metadata": {
        "id": "OOXfGlCXe7b_"
      },
      "id": "OOXfGlCXe7b_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANSWER**\n",
        "\n",
        "## A.\n",
        "**As we see in the above results the TFIDF has shown the best result among the vectoriser we tried out that is count vectorizer.**\n",
        "\n",
        "**In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.**\n",
        "\n",
        "\n",
        "**Word2Vec and Glove were also tried to represent sentences using word embeddings but it resulted in ever poorer results and hence commented in the code.**\n",
        "\n",
        "## B \n",
        "\n",
        "**Among the various models we tried the linear regression model was better than SVC and naive baiyes in the above scenario.**\n",
        "\n",
        "**As we see below the linear regression has better accuracy, f1 score,recall and precession than SVC and NB**\n",
        "\n",
        "\n",
        "          displaying  metrics for the mode OneVsRestClassifier(estimator=LogisticRegression(penalty='l1',\n",
        "                                                          solver='liblinear'))\n",
        "\n",
        "          Accuracy score:  0.14985\n",
        "          F1 score: Micro 0.5055058388749826\n",
        "          Average precision score: Micro 0.30164968347917864\n",
        "          Average recall score: Micro 0.39211666666666667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          Accuracy score:  0.14985\n",
        "          F1 score: Macro 0.2228209587635938\n",
        "          Average recall score: MAcro 0.1610025184789498\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          Accuracy score:  0.14985\n",
        "          F1 score: weighted 0.4857228587961707\n",
        "          Average precision score: weighted 0.40477645040650023\n",
        "          Average recall score: weighted 0.39211666666666667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          **displaying  metrics for the mode OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1'))\n",
        "\n",
        "          Accuracy score:  0.13255\n",
        "          F1 score: Micro 0.47161826937107837\n",
        "          Average precision score: Micro 0.277235380957292\n",
        "          Average recall score: Micro 0.35013333333333335\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          Accuracy score:  0.13255\n",
        "          F1 score: Macro 0.22091840140454916\n",
        "          Average recall score: MAcro 0.15200668106809853\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          Accuracy score:  0.13255\n",
        "          F1 score: weighted 0.45544778684081505\n",
        "          Average precision score: weighted 0.39442506017285967\n",
        "          Average recall score: weighted 0.35013333333333335\n",
        "\n",
        "## C\n",
        "\n",
        "****We have used one vs rest classsifier for tuning and tht parameter for each model is found**\n",
        "\n",
        "**We have used different parameter for the tunining. Due to high execution time and limited CPU with google collab pro, we could check with limited parameters.(Execution taking more than 4-5 hours for one model only)**\n",
        "\n",
        "**Best score is given by solver = lbfgsm, penalty = l1, and C=1 since they have best accuracy**\n",
        "\n",
        "****"
      ],
      "metadata": {
        "id": "qHUUftK7tnnL"
      },
      "id": "qHUUftK7tnnL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## D\n",
        "\n",
        "In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently.\n",
        "\n",
        "The classification report displays the precision, recall, F1, and support scores for the model.\n",
        "\n",
        "Precision: Precision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”\n",
        "\n",
        "Recall : Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”\n",
        "\n",
        "F1-Score:The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.Similar to arithmetic mean, the F1-score will always be somewhere in between precision and mean. But it behaves differently: the F1-score gives a larger weight to lower numbers. For example, when Precision is 100% and Recall is 0%, the F1-score will be 0%, not 50%. Or for example, say that Classifier A has precision=recall=80%, and Classifier B has precision=60%, recall=100%. Arithmetically, the mean of the precision and recall is the same for both models. But when we use F1’s harmonic mean formula, the score for Classifier A will be 80%, and for Classifier B it will be only 75%. Model B’s low precision score pulled down its F1-score.\n",
        "\n",
        "Support : Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process.\n",
        "\n",
        "Macro-averaged : Combining the per-class F1-scores into a single number, the classifier’s overall F1-score. There are a few ways of doing that. Let’s begin with the simplest one: an arithmetic mean of the per-class F1-scores. This is called the macro-averaged F1-score, or the macro-F1 for short, and is computed as a simple arithmetic mean of our per-class F1-scores: Macro-F1 = (42.1% + 30.8% + 66.7%) / 3 = 46.5% In a similar way, we can also compute the macro-averaged precision and the macro-averaged recall: Macro-precision = (31% + 67% + 67%) / 3 = 54.7% Macro-recall = (67% + 20% + 67%) / 3 = 51.1%\n",
        "\n",
        "Weighted Ang: When averaging the macro-F1, we gave equal weights to each class. We don’t have to do that: in weighted-average F1-score, or weighted-F1, we weight the F1-score of each class by the number of samples from that class. In our case, we have a total of 25 samples: 6 Cat, 10 Fish, and 9 Hen. The weighted-F1 score is thus computed as follows: Weighted-F1 = (6 × 42.1% + 10 × 30.8% + 9 × 66.7%) / 25 = 46.4% Similarly, we can compute weighted precision and weighted recall: Weighted-precision=(6 × 30.8% + 10 × 66.7% + 9 × 66.7%)/25 = 58.1% Weighted-recall = (6 × 66.7% + 10 × 20.0% + 9 × 66.7%) / 25 = 48.0%\n",
        "\n",
        "Micro Average: The last variant is the micro-averaged F1-score, or the micro-F1. To calculate the micro-F1, we first compute micro-averaged precision and micro-averaged recall over all the samples , and then combine the two. How do we “micro-average”? We simply look at all the samples together. Remember that precision is the proportion of True Positives out of the Predicted Positives (TP/(TP+FP)). In the multi-class case, we consider all the correctly predicted samples to be True Positives"
      ],
      "metadata": {
        "id": "0vYN03yyeywn"
      },
      "id": "0vYN03yyeywn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------\n",
        "---------------------------------------------\n",
        "\n",
        "# PART TWO\n",
        "-------------------------------\n",
        "--------------------------------"
      ],
      "metadata": {
        "id": "izvMV5tuynLq"
      },
      "id": "izvMV5tuynLq"
    },
    {
      "cell_type": "markdown",
      "source": [
        " **DOMAIN:** Customer support\n",
        "\n",
        "• **CONTEXT**: Great Learning has a an academic support department which receives numerous support requests every day throughout the year. Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with\n",
        "the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request to an actual human support executive if the request is complex or not in it’s database.\n",
        "\n",
        "• **DATA DESCRIPTION:** A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills.\n",
        "\n",
        "• **PROJECT OBJECTIVE:** Design a python based interactive semi - rule based chatbot which can do the following:\n",
        "1. Start chat session with greetings and ask what the user is looking for. [5 Marks]\n",
        "2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus. [10 Marks]\n",
        "3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it. [5 Marks]\n",
        "Hint: There are a lot of techniques using which one can clean and prepare the data which can be used to train a ML/DL classifier. Hence, it might\n",
        "require you to experiment, research, self learn and implement the above classifier. There might be many iterations between hand building the\n",
        "corpus and designing the best fit text classifier. As the quality and quantity of corpus increases the model’s performance i.e. ability to answer\n",
        "right questions also increases.\n",
        " Reference: https://www.mygreatlearning.com/blog/basics-of-building-an-artificial-intelligence-chatbot/\n",
        "\n",
        "• **Evaluation:** Evaluator will use linguistics to twist and turn sentences to ask questions on the topics described in DATA DESCRIPTION and check if\n",
        "the bot is giving relevant replies."
      ],
      "metadata": {
        "id": "wfGsmzkJy2y2"
      },
      "id": "wfGsmzkJy2y2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the packages**"
      ],
      "metadata": {
        "id": "VhOf9dLD0oeN"
      },
      "id": "VhOf9dLD0oeN"
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "C8x0yIdNe0bQ"
      },
      "id": "C8x0yIdNe0bQ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading in the corpus**\n",
        "\n",
        "For our example,we will be using the file provided \"GL Bot (1).json\" for chatbots as our corpus.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0lS6LxH0pFB"
      },
      "id": "Y0lS6LxH0pFB"
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"/content/drive/My Drive/aiml/nlp/project1/\"\n",
        "\n",
        "with open(project_path+'GL Bot (1).json') as file:\n",
        "    data = json.load(file)\n",
        "    \n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent['tag'])\n",
        "    responses.append(intent['responses'])\n",
        "    \n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "        \n",
        "num_classes = len(labels)"
      ],
      "metadata": {
        "id": "n3nkN0cNzpKw"
      },
      "id": "n3nkN0cNzpKw",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The variable “training_sentences” holds all the training data (which are the sample messages in each intent category) and the “training_labels” variable holds all the target labels correspond to each training data.**\n",
        "\n",
        "**Then we use “LabelEncoder()” function provided by scikit-learn to convert the target labels into a model understandable form.**\n",
        "\n",
        "### Tokenisation\n",
        "\n"
      ],
      "metadata": {
        "id": "DX0K71yd1CV3"
      },
      "id": "DX0K71yd1CV3"
    },
    {
      "cell_type": "code",
      "source": [
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)\n",
        "\n",
        "\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "6jwIB5fT0CcS"
      },
      "id": "6jwIB5fT0CcS",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "\n",
        "**Let’s define our Neural Network architecture for the proposed model and for that we use the “Sequential” model class of Keras.**"
      ],
      "metadata": {
        "id": "Qt1c9VeW1Lyq"
      },
      "id": "Qt1c9VeW1Lyq"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5N9kjPB0FKi",
        "outputId": "42bc4e2e-6df2-470b-8e0c-7cf0d094b23a"
      },
      "id": "l5N9kjPB0FKi",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 16)            16000     \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                272       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,680\n",
            "Trainable params: 16,680\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we are ready to train our model. Simply we can call the “fit” method with training data and labels.**"
      ],
      "metadata": {
        "id": "pzF272j42Ksv"
      },
      "id": "pzF272j42Ksv"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJta7FvT0JtM",
        "outputId": "70fb1a97-0ef9-4ceb-b333-dfbb28afc323"
      },
      "id": "DJta7FvT0JtM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 2.0787 - accuracy: 0.2109\n",
            "Epoch 2/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0754 - accuracy: 0.2500\n",
            "Epoch 3/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0726 - accuracy: 0.2656\n",
            "Epoch 4/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0698 - accuracy: 0.2812\n",
            "Epoch 5/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0668 - accuracy: 0.3047\n",
            "Epoch 6/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0634 - accuracy: 0.2422\n",
            "Epoch 7/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0596 - accuracy: 0.2266\n",
            "Epoch 8/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0562 - accuracy: 0.2266\n",
            "Epoch 9/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0521 - accuracy: 0.2266\n",
            "Epoch 10/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0479 - accuracy: 0.2266\n",
            "Epoch 11/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0437 - accuracy: 0.2266\n",
            "Epoch 12/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0386 - accuracy: 0.2266\n",
            "Epoch 13/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0344 - accuracy: 0.2266\n",
            "Epoch 14/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0299 - accuracy: 0.2266\n",
            "Epoch 15/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0248 - accuracy: 0.2266\n",
            "Epoch 16/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 2.0193 - accuracy: 0.2266\n",
            "Epoch 17/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0135 - accuracy: 0.2266\n",
            "Epoch 18/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0088 - accuracy: 0.2266\n",
            "Epoch 19/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 2.0031 - accuracy: 0.2266\n",
            "Epoch 20/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9969 - accuracy: 0.2266\n",
            "Epoch 21/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9912 - accuracy: 0.2266\n",
            "Epoch 22/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9854 - accuracy: 0.2266\n",
            "Epoch 23/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9789 - accuracy: 0.2266\n",
            "Epoch 24/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9729 - accuracy: 0.2266\n",
            "Epoch 25/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9666 - accuracy: 0.2266\n",
            "Epoch 26/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9600 - accuracy: 0.2266\n",
            "Epoch 27/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9543 - accuracy: 0.2266\n",
            "Epoch 28/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9484 - accuracy: 0.2266\n",
            "Epoch 29/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9423 - accuracy: 0.2266\n",
            "Epoch 30/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9365 - accuracy: 0.2266\n",
            "Epoch 31/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9299 - accuracy: 0.2266\n",
            "Epoch 32/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9248 - accuracy: 0.2266\n",
            "Epoch 33/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9174 - accuracy: 0.2266\n",
            "Epoch 34/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9107 - accuracy: 0.2266\n",
            "Epoch 35/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.9039 - accuracy: 0.2266\n",
            "Epoch 36/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8970 - accuracy: 0.2266\n",
            "Epoch 37/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.8895 - accuracy: 0.2266\n",
            "Epoch 38/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.8818 - accuracy: 0.2266\n",
            "Epoch 39/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8736 - accuracy: 0.2266\n",
            "Epoch 40/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8658 - accuracy: 0.2344\n",
            "Epoch 41/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8567 - accuracy: 0.2422\n",
            "Epoch 42/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8479 - accuracy: 0.2500\n",
            "Epoch 43/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8398 - accuracy: 0.2578\n",
            "Epoch 44/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8298 - accuracy: 0.2578\n",
            "Epoch 45/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8200 - accuracy: 0.2578\n",
            "Epoch 46/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.8099 - accuracy: 0.2578\n",
            "Epoch 47/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7991 - accuracy: 0.2656\n",
            "Epoch 48/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7882 - accuracy: 0.2812\n",
            "Epoch 49/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7783 - accuracy: 0.3047\n",
            "Epoch 50/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7658 - accuracy: 0.3125\n",
            "Epoch 51/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7550 - accuracy: 0.3047\n",
            "Epoch 52/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7429 - accuracy: 0.3047\n",
            "Epoch 53/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7304 - accuracy: 0.3125\n",
            "Epoch 54/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7182 - accuracy: 0.3125\n",
            "Epoch 55/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.7059 - accuracy: 0.3125\n",
            "Epoch 56/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.6932 - accuracy: 0.3516\n",
            "Epoch 57/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.6797 - accuracy: 0.3828\n",
            "Epoch 58/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.6669 - accuracy: 0.3672\n",
            "Epoch 59/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.6535 - accuracy: 0.3359\n",
            "Epoch 60/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.6389 - accuracy: 0.3906\n",
            "Epoch 61/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.6243 - accuracy: 0.3984\n",
            "Epoch 62/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.6115 - accuracy: 0.3984\n",
            "Epoch 63/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5950 - accuracy: 0.3984\n",
            "Epoch 64/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5805 - accuracy: 0.3984\n",
            "Epoch 65/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5663 - accuracy: 0.3984\n",
            "Epoch 66/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5508 - accuracy: 0.4062\n",
            "Epoch 67/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5363 - accuracy: 0.4062\n",
            "Epoch 68/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5218 - accuracy: 0.4141\n",
            "Epoch 69/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.5050 - accuracy: 0.4141\n",
            "Epoch 70/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.4903 - accuracy: 0.4297\n",
            "Epoch 71/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.4738 - accuracy: 0.4297\n",
            "Epoch 72/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.4591 - accuracy: 0.4297\n",
            "Epoch 73/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.4426 - accuracy: 0.4297\n",
            "Epoch 74/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.4284 - accuracy: 0.4375\n",
            "Epoch 75/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.4120 - accuracy: 0.4609\n",
            "Epoch 76/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3956 - accuracy: 0.4844\n",
            "Epoch 77/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3774 - accuracy: 0.4922\n",
            "Epoch 78/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3607 - accuracy: 0.4844\n",
            "Epoch 79/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3438 - accuracy: 0.5078\n",
            "Epoch 80/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3256 - accuracy: 0.5703\n",
            "Epoch 81/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.3092 - accuracy: 0.5938\n",
            "Epoch 82/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.2921 - accuracy: 0.6016\n",
            "Epoch 83/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.2766 - accuracy: 0.6016\n",
            "Epoch 84/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.2599 - accuracy: 0.6016\n",
            "Epoch 85/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.2426 - accuracy: 0.6016\n",
            "Epoch 86/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.2267 - accuracy: 0.6016\n",
            "Epoch 87/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.2105 - accuracy: 0.6094\n",
            "Epoch 88/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1941 - accuracy: 0.6250\n",
            "Epoch 89/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1786 - accuracy: 0.6250\n",
            "Epoch 90/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1619 - accuracy: 0.6328\n",
            "Epoch 91/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1456 - accuracy: 0.6406\n",
            "Epoch 92/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1295 - accuracy: 0.6484\n",
            "Epoch 93/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1133 - accuracy: 0.6484\n",
            "Epoch 94/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0976 - accuracy: 0.6484\n",
            "Epoch 95/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0819 - accuracy: 0.6484\n",
            "Epoch 96/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0645 - accuracy: 0.6641\n",
            "Epoch 97/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0494 - accuracy: 0.6641\n",
            "Epoch 98/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0322 - accuracy: 0.6719\n",
            "Epoch 99/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0152 - accuracy: 0.6719\n",
            "Epoch 100/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.6875\n",
            "Epoch 101/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9830 - accuracy: 0.6875\n",
            "Epoch 102/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9657 - accuracy: 0.6875\n",
            "Epoch 103/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9492 - accuracy: 0.7188\n",
            "Epoch 104/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9327 - accuracy: 0.7500\n",
            "Epoch 105/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9164 - accuracy: 0.7500\n",
            "Epoch 106/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.9000 - accuracy: 0.7500\n",
            "Epoch 107/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.8826 - accuracy: 0.7500\n",
            "Epoch 108/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.8655 - accuracy: 0.7656\n",
            "Epoch 109/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.8492 - accuracy: 0.7656\n",
            "Epoch 110/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.8319 - accuracy: 0.7656\n",
            "Epoch 111/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.8140 - accuracy: 0.7656\n",
            "Epoch 112/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7987 - accuracy: 0.7734\n",
            "Epoch 113/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7815 - accuracy: 0.7734\n",
            "Epoch 114/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7641 - accuracy: 0.7969\n",
            "Epoch 115/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7474 - accuracy: 0.8125\n",
            "Epoch 116/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7304 - accuracy: 0.8203\n",
            "Epoch 117/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7144 - accuracy: 0.8359\n",
            "Epoch 118/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6976 - accuracy: 0.8438\n",
            "Epoch 119/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.8438\n",
            "Epoch 120/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6653 - accuracy: 0.8672\n",
            "Epoch 121/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.8672\n",
            "Epoch 122/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.8750\n",
            "Epoch 123/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.9062\n",
            "Epoch 124/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.9297\n",
            "Epoch 125/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.9297\n",
            "Epoch 126/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.9375\n",
            "Epoch 127/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.9531\n",
            "Epoch 128/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.9531\n",
            "Epoch 129/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.9531\n",
            "Epoch 130/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.9531\n",
            "Epoch 131/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.9531\n",
            "Epoch 132/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.9531\n",
            "Epoch 133/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.9688\n",
            "Epoch 134/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.9688\n",
            "Epoch 135/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.9688\n",
            "Epoch 136/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.9688\n",
            "Epoch 137/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.9688\n",
            "Epoch 138/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.9688\n",
            "Epoch 139/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.9688\n",
            "Epoch 140/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.9688\n",
            "Epoch 141/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.9688\n",
            "Epoch 142/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.9688\n",
            "Epoch 143/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.9766\n",
            "Epoch 144/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.9844\n",
            "Epoch 145/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.9844\n",
            "Epoch 146/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.9844\n",
            "Epoch 147/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.9844\n",
            "Epoch 148/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.9844\n",
            "Epoch 149/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.9844\n",
            "Epoch 150/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.9844\n",
            "Epoch 151/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9844\n",
            "Epoch 152/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9844\n",
            "Epoch 153/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9844\n",
            "Epoch 154/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9844\n",
            "Epoch 155/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9844\n",
            "Epoch 156/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9844\n",
            "Epoch 157/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9844\n",
            "Epoch 158/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9844\n",
            "Epoch 159/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9844\n",
            "Epoch 160/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9844\n",
            "Epoch 161/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9922\n",
            "Epoch 162/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9922\n",
            "Epoch 163/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9922\n",
            "Epoch 164/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1740 - accuracy: 0.9922\n",
            "Epoch 165/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9922\n",
            "Epoch 166/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1641 - accuracy: 0.9922\n",
            "Epoch 167/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9922\n",
            "Epoch 168/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9922\n",
            "Epoch 169/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1510 - accuracy: 0.9922\n",
            "Epoch 170/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9922\n",
            "Epoch 171/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9922\n",
            "Epoch 172/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9922\n",
            "Epoch 173/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9922\n",
            "Epoch 174/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1321 - accuracy: 0.9922\n",
            "Epoch 175/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 1.0000\n",
            "Epoch 176/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9922\n",
            "Epoch 177/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9922\n",
            "Epoch 178/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 1.0000\n",
            "Epoch 179/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 1.0000\n",
            "Epoch 180/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 1.0000\n",
            "Epoch 181/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9922\n",
            "Epoch 182/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1077 - accuracy: 1.0000\n",
            "Epoch 183/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 1.0000\n",
            "Epoch 184/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 1.0000\n",
            "Epoch 185/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.1000 - accuracy: 1.0000\n",
            "Epoch 186/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 1.0000\n",
            "Epoch 187/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0951 - accuracy: 1.0000\n",
            "Epoch 188/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0931 - accuracy: 1.0000\n",
            "Epoch 189/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 1.0000\n",
            "Epoch 190/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 1.0000\n",
            "Epoch 191/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 1.0000\n",
            "Epoch 192/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 1.0000\n",
            "Epoch 193/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0828 - accuracy: 1.0000\n",
            "Epoch 194/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 1.0000\n",
            "Epoch 195/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 1.0000\n",
            "Epoch 196/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 1.0000\n",
            "Epoch 197/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 1.0000\n",
            "Epoch 198/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 1.0000\n",
            "Epoch 199/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 1.0000\n",
            "Epoch 200/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 1.0000\n",
            "Epoch 201/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 1.0000\n",
            "Epoch 202/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 1.0000\n",
            "Epoch 203/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 1.0000\n",
            "Epoch 204/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 1.0000\n",
            "Epoch 205/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 1.0000\n",
            "Epoch 206/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 1.0000\n",
            "Epoch 207/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 1.0000\n",
            "Epoch 208/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 1.0000\n",
            "Epoch 209/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 1.0000\n",
            "Epoch 210/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 1.0000\n",
            "Epoch 211/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 1.0000\n",
            "Epoch 212/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 1.0000\n",
            "Epoch 213/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 1.0000\n",
            "Epoch 214/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 1.0000\n",
            "Epoch 215/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 1.0000\n",
            "Epoch 216/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 1.0000\n",
            "Epoch 217/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 218/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 1.0000\n",
            "Epoch 219/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 1.0000\n",
            "Epoch 220/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 1.0000\n",
            "Epoch 221/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 1.0000\n",
            "Epoch 222/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 1.0000\n",
            "Epoch 223/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 1.0000\n",
            "Epoch 224/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 1.0000\n",
            "Epoch 225/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 1.0000\n",
            "Epoch 226/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 1.0000\n",
            "Epoch 227/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 1.0000\n",
            "Epoch 228/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 229/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 230/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 1.0000\n",
            "Epoch 231/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 232/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 233/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 234/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 235/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 236/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 237/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 238/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 1.0000\n",
            "Epoch 239/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 240/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 241/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 242/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 1.0000\n",
            "Epoch 243/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 244/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 245/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 246/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 1.0000\n",
            "Epoch 247/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 1.0000\n",
            "Epoch 248/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 249/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 250/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 251/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 1.0000\n",
            "Epoch 252/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 253/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 1.0000\n",
            "Epoch 254/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 255/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 256/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 257/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 258/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 259/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 1.0000\n",
            "Epoch 260/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 261/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 262/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 1.0000\n",
            "Epoch 263/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 1.0000\n",
            "Epoch 264/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 265/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 266/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 267/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 268/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 269/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 1.0000\n",
            "Epoch 270/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 271/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 1.0000\n",
            "Epoch 272/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 1.0000\n",
            "Epoch 273/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 274/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 1.0000\n",
            "Epoch 275/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 1.0000\n",
            "Epoch 276/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 277/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 1.0000\n",
            "Epoch 278/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 279/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 280/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 281/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 1.0000\n",
            "Epoch 282/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 1.0000\n",
            "Epoch 283/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 284/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 1.0000\n",
            "Epoch 285/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 286/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 1.0000\n",
            "Epoch 287/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 1.0000\n",
            "Epoch 288/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 289/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 290/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 291/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 292/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 293/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 294/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000\n",
            "Epoch 295/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 1.0000\n",
            "Epoch 296/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 297/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 298/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 1.0000\n",
            "Epoch 299/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 300/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 1.0000\n",
            "Epoch 301/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 302/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 303/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 304/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 305/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 306/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 307/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 308/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 309/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 310/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 311/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 312/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 313/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 314/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 315/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 316/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 317/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 318/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 319/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 320/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 321/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 322/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 323/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 324/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 325/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 326/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 327/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 328/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 329/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 330/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 332/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 333/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 334/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 335/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 336/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 337/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 338/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 339/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 340/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 341/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 342/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 343/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 344/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 345/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 346/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 347/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 348/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 349/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 350/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 351/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 352/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 353/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 354/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 355/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 356/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 357/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 358/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 359/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 360/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 361/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 362/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 363/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 364/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 365/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 366/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 367/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 368/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 369/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 370/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 371/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 372/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 373/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 374/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 375/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 376/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 377/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 378/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 379/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 380/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 381/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 382/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 383/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 384/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 385/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 386/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 387/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 388/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 389/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 390/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 391/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 392/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 393/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 394/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 395/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 396/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 397/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 398/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 399/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 400/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 401/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 402/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 403/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 404/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 405/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 406/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 407/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 408/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 409/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 410/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 411/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 412/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 413/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 414/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 415/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 416/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 417/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 418/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 419/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 420/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 421/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 422/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 423/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 424/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 425/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 426/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 427/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 428/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 429/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 430/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 431/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 432/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 433/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 434/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 435/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 436/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 437/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 438/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 439/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 440/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 441/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 442/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 443/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 444/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 445/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 446/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 447/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 448/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 449/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 450/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 451/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 452/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 453/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 454/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 456/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 458/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 463/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 494/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 496/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 498/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 500/500\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After training, it is better to save all the required files in order to use it at the inference time. So that we save the trained model, fitted tokenizer object and fitted label encoder object.**"
      ],
      "metadata": {
        "id": "CmbqfX4Jjxix"
      },
      "id": "CmbqfX4Jjxix"
    },
    {
      "cell_type": "code",
      "source": [
        "# to save the trained model\n",
        "model.save(\"chat_model\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNpcFV4HhoIz",
        "outputId": "ef315e36-5d99-45e6-ecb4-044ae9505a55"
      },
      "id": "eNpcFV4HhoIz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: chat_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are going to implement a chat function to engage with a real user. When a new user message is received, the chatbot will calculate the similarity between the new text sequence and training data.**\n",
        "\n"
      ],
      "metadata": {
        "id": "TdHPNDTi2Xy9"
      },
      "id": "TdHPNDTi2Xy9"
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import colorama \n",
        "colorama.init()\n",
        "from colorama import Fore, Style, Back\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'GL Bot (1).json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "def chat():\n",
        "    # load trained model\n",
        "    model = keras.models.load_model('chat_model')\n",
        "\n",
        "    # load tokenizer object\n",
        "    with open('tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # load label encoder object\n",
        "    with open('label_encoder.pickle', 'rb') as enc:\n",
        "        lbl_encoder = pickle.load(enc)\n",
        "\n",
        "    # parameters\n",
        "    max_len = 20\n",
        "    \n",
        "    while True:\n",
        "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
        "                                             truncating='post', maxlen=max_len))\n",
        "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "\n",
        "        for i in data['intents']:\n",
        "            if i['tag'] == tag:\n",
        "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
        "\n",
        "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
        "\n",
        "print(Fore.YELLOW + \"Hi!! I am assistant for greatleaning. Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7eNnmJ3dEu9",
        "outputId": "f1c03edb-4334-4434-cb78-5076f0d6536a"
      },
      "id": "p7eNnmJ3dEu9",
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi!! I am assistant for greatleaning. Start messaging with the bot (type quit to stop)!\n",
            "User: Hi\n",
            "ChatBot: Hello! how can i help you ?\n",
            "User: explain me how olympus works\n",
            "ChatBot: Link: Olympus wiki\n",
            "User: i am not able to understand svm\n",
            "ChatBot: Link: Machine Learning wiki \n",
            "User: understand svm\n",
            "ChatBot: Link: Machine Learning wiki \n",
            "User: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot is working as expected**"
      ],
      "metadata": {
        "id": "f1ctIDw1kg77"
      },
      "id": "f1ctIDw1kg77"
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------\n",
        "\n",
        "# END\n",
        "\n",
        "------------------------------------"
      ],
      "metadata": {
        "id": "LJCOS4cnkYQC"
      },
      "id": "LJCOS4cnkYQC"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BZQ0zSayhxM9"
      },
      "id": "BZQ0zSayhxM9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Project1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}